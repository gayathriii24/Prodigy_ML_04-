{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PyuW4XxaO2kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile # Import zipfile module for extraction\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# Set dataset path\n",
        "data_dir = r\"C:\\Users\\Gayathri Poojary\\Downloads\\archive (6).zip\"\n",
        "IMG_SIZE = 64  # Resize all images to 64x64\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Function to load and preprocess the dataset\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(data_dir, 'r') as zip_ref:\n",
        "        zip_ref.extractall(r'C:\\Users\\Gayathri Poojary\\Downloads\\archive (6).zip') # Extract to /content/train directory\n",
        "\n",
        "\n",
        "    extracted_data_dir = r'C:\\Users\\Gayathri Poojary\\Downloads\\archive (6).zip'  # Navigate to the gesture subfolder\n",
        "\n",
        "    for gesture_folder in tqdm(os.listdir(extracted_data_dir)):\n",
        "        if not gesture_folder.split('_')[0].isdigit():\n",
        "            continue\n",
        "\n",
        "        gesture_path = os.path.join(extracted_data_dir, gesture_folder)\n",
        "        # Ensure label is within the valid range (0-9)\n",
        "        label = int(gesture_folder.split('_')[0]) % NUM_CLASSES  # Extract label from folder name and apply modulo\n",
        "\n",
        "        for img_file in os.listdir(gesture_path):\n",
        "            img_path = os.path.join(gesture_path, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "\n",
        "    images = np.array(images, dtype=\"float32\") / 255.0  # Normalize pixel values\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n",
        "# Load dataset\n",
        "print(\"Loading data...\")\n",
        "X, y = load_data(data_dir)\n",
        "X = X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)  # Add channel dimension for grayscale images\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(\n",
        "    data_gen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=3\n",
        ")\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating the model...\")\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(classification_report(y_test, y_pred, target_names=[f\"Gesture {i}\" for i in range(NUM_CLASSES)]))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"hand_gesture_model.h5\")\n",
        "print(\"Model saved as hand_gesture_model.h5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ca96a2-fa8a-431f-b26d-2f5ac287014c",
        "id": "Fl8OFcIcOg_r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:01<00:00, 11.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded.\n",
            "Training the model...\n",
            "Epoch 1/3\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 266ms/step - accuracy: 0.4889 - loss: 1.4254 - val_accuracy: 0.9992 - val_loss: 0.0401\n",
            "Epoch 2/3\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 264ms/step - accuracy: 0.8406 - loss: 0.4526 - val_accuracy: 0.9994 - val_loss: 0.0112\n",
            "Epoch 3/3\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 256ms/step - accuracy: 0.8938 - loss: 0.3105 - val_accuracy: 0.9994 - val_loss: 0.0094\n",
            "Model training completed.\n",
            "Evaluating the model...\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Gesture 0       1.00      1.00      1.00       362\n",
            "   Gesture 1       1.00      1.00      1.00       362\n",
            "   Gesture 2       1.00      1.00      1.00       368\n",
            "   Gesture 3       1.00      1.00      1.00       371\n",
            "   Gesture 4       1.00      1.00      1.00       342\n",
            "   Gesture 5       1.00      1.00      1.00       355\n",
            "   Gesture 6       1.00      1.00      1.00       353\n",
            "   Gesture 7       1.00      1.00      1.00       363\n",
            "   Gesture 8       1.00      1.00      1.00       383\n",
            "   Gesture 9       1.00      1.00      1.00       341\n",
            "\n",
            "    accuracy                           1.00      3600\n",
            "   macro avg       1.00      1.00      1.00      3600\n",
            "weighted avg       1.00      1.00      1.00      3600\n",
            "\n",
            "Model saved as hand_gesture_model.h5.\n"
          ]
        }
      ]
    }
  ]
}